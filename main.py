import time
import ssl
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from fix_mysql import run_emergency_fix

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
from db_service import create_connection, create_tables
from book_history import update_history

# Scrapers
from crawlers.amarin import scrape_amarin_all_pages
from crawlers.b2s import scrape_b2s_all_pages
from crawlers.jamsai import scrape_jamsai_all_pages
from crawlers.niin import scrape_naiin_all_pages
from crawlers.seed import scrape_seed_all_pages 

ssl._create_default_https_context = ssl._create_unverified_context

def get_driver():
    """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Chrome Driver ‡πÉ‡∏´‡πâ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ö‡∏ô Server (Headless)"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def main():
    run_emergency_fix() 
    # ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    conn = create_connection()
    if conn is None:
        print("[ERROR] ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ MySQL ‡πÑ‡∏î‡πâ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Variables ‡∏ö‡∏ô Railway")
        return

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏≤‡∏£‡∏≤‡∏á
    create_tables(conn)

    # ‡πÄ‡∏õ‡∏¥‡∏î Browser
    driver = get_driver()
    
    # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡πá‡∏ö‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏õ‡∏î‡∏∂‡∏á
    sites = [
        {"name": "niin", "scraper": scrape_naiin_all_pages},
        {"name": "se-ed", "scraper": scrape_seed_all_pages},
        {"name": "jamsai", "scraper": scrape_jamsai_all_pages},
        {"name": "b2s", "scraper": scrape_b2s_all_pages},
        {"name": "amarin", "scraper": scrape_amarin_all_pages},
    ]

    max_pages_to_scrape = 100

    for site in sites:
        source = site['name']
        scraper_func = site['scraper']
        
        print(f"\nüöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å: {source.upper()}")
        try:
            scraper_func(driver, conn, max_pages=max_pages_to_scrape)
        except Exception as e:
            print(f"[ERROR] ‡πÄ‡∏ß‡πá‡∏ö {source} ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤: {e}")
            continue

    print("\nüìä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏£‡∏≤‡∏Ñ‡∏≤ (update_history)...")
    update_history(conn)

    print("\n‚ú® ‡∏†‡∏≤‡∏£‡∏Å‡∏¥‡∏à‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á Database ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ Supabase ‡πÅ‡∏•‡πâ‡∏ß")
    
    driver.quit()
    conn.close()

if __name__ == "__main__":
    main()